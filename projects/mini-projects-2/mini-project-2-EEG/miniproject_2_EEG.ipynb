{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01e5b57-623f-4004-87a8-600c49e90f97",
   "metadata": {},
   "source": [
    "# Programming for Data Analysis (2022)\n",
    "**Arthur Leblois & Nicolas P. Rougier**, Institute of Neurodegenerative Diseases, Bordeaux, France.  \n",
    "Course material and program at https://github.com/bordeaux-neurocampus/UE-Programming-for-data-analysis-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3b6d5f-ef63-4517-a1fc-640c8ab1f4c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<img style=\"align:center;\" src=\"figures/An-Overview-of-EEG-data-clusters-used-in-analysis-and-B-randomly-selected-EEG-time.png\">\n",
    "\n",
    "\n",
    "## Mini-Project 2 // EEG: Analyzing signals from the brain!\n",
    "\n",
    "The goal of this project is to learn how to manipulate EEG data. In EEG research, recorded raw data that are used in analyses require preparation, also called preprocessing, a process that involves a number of manipulations. During this project you will go over a few key points regarding data preprocessing and analysis.\n",
    "\n",
    "Some common brain rhythms recorded in the EEG are _alpha_ ($\\alpha$), _beta_ ($\\beta$), _gamma_ ($\\gamma$), _delta_ ($\\delta$), and _theta_ ($\\theta$). These rhythms are identified by frequency (Hz or cycles/sec) and amplitude and each of them is linked to a different phase of processing in the brain. You can read more in the [EEG introductory lesson](https://www.csun.edu/~vcpsy00i/dissfa01/xEEG_lesson.html).\n",
    "\n",
    "The data that you will be working with belongs to a single participant performing multiple experimental conditions while wearing a VR headset. During the experimental session, the participant was asked to perform a version of the [Paired Associates Learning (PAL)](https://www.cambridgecognition.com/cantab/cognitive-tests/memory/paired-associates-learning-pal/) task, which is used by clinicians to assess visual memory and new learning in humans. Audio/Visual stimulation was also tested on the participant.\n",
    "\n",
    "We will take a closer look on the data collected during the equipment calibration step and assess the baseline brain activity of the participant with eyes open and eyes closed (no stimulation). One of the goals of the project is to see if you can identify the subtle differences on the EEG traces and the underlying rhythms.\n",
    "\n",
    "Since we will be manipulating EEG data, we will have to perform preliminary operations to prepare the dataset for further analyses. One interesting resource is [Makoto's preprocessing pipeline](https://sccn.ucsd.edu/wiki/Makoto%27s_preprocessing_pipeline). This pipeline is developed by Makoto Miyakoshi, affiliated with the [Swartz Center for Computational Neuroscience](https://sccn.ucsd.edu/) and - while mainly developed as a guide for using EEGlab in Matlab - contains a lot of useful information that you should refer to whenever you feel like you do not understand the **what** or **why** we are performing a computation.\n",
    "\n",
    "**Data** is available as [EEGlab](https://sccn.ucsd.edu/eeglab/index.php) `.set` files in the `data` subfolder. Before you start, please make sure the data directory is under `data` and named `EEG_raw`.\n",
    "\n",
    "---\n",
    "\n",
    "**\\[Exercises\\]** This section contains the exercises you will need to complete.\n",
    "\n",
    "**(Hint)** For every exercise, hints will be provided that will point you to the right direction. You **will** have to read the documentation and online resources often! If you need extra help, [this blog](https://neuraldatascience.io/7-eeg/mne_python.html) contains a lot of useful information and explanations. Use all of the resources available to you to complete the project!\n",
    "\n",
    "**Answers:** You can edit these cells directly and submit your answers as this completed notebook. Please include your team number in the filename i.e. `Team_4_project_2_EEG.ipynb`.\n",
    "\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "* [1. Configuration of the notebook](#1.-Configuration-of-the-notebook)\n",
    "* [2. Loading libraries](#2.-Loading-libraries)\n",
    "* [3. Loading data](#3.-Loading-data)\n",
    "* [4. Visualizing data](#4.-Visualizing-data)\n",
    "* [5. Preprocessing](#5.-Preprocessing)\n",
    "* [6. Overview](#6.-Overview)\n",
    "* [7. A simple analysis](#7.-A-simple-analysis)\n",
    "* [8. Comparison of eyes open vs eyes closed](#8.-Comparison-of-eyes-open-vs-eyes-closed)\n",
    "* [9. Outro](#9.-Outro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291ff4e-234a-474a-9d0b-ad4552f25b6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Configuration of the notebook\n",
    "\n",
    "We need first to setup a few options in the notebook such as to have inline plots as well as a nicer output on OSX.  \n",
    "There is not much to understand here, these options are documented in the [Jupyer notebook documentation](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae2e5c8-1479-4eba-b2e7-90344d4dc45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask jupyter to display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# OSX specific (for a nicer display on \"retina\" screen)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# mne.viz.set_3d_backend(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76673579-ba32-4fb7-a4f8-226d770ec818",
   "metadata": {},
   "source": [
    "**Reminder**: In order to run the code in a specific code cell, you'll have to type `ctrl`+`return` on the selected cell. If you want to run the code and go to the next cell, you'll have to tyle `shift`+`return` (creates a new empty cell if there isn't one below the current cell). If you do that manually, you'll have to run each cell from top to bottom (order is important). If you want to run all the cell, you can also click the run button at the top of the notebook. To edit a cell (code or text), double-click in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a241a7-7e24-452e-8096-0113f369b001",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Loading libraries\n",
    "\n",
    "Next step is to load all the Python libraries that will be needed for processing & displaying our data. Namely:\n",
    "* [NumPy](https://www.numpy.org/) which is the fundamental package for scientific computing with Python.\n",
    "* [MNE](https://mne.tools/stable/index.html) is the main EEG analysis toolbox that we will use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f3f06a-7a9f-4c82-aedd-5803d9ee62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating System calls, used to standardize file I/O across operating systems\n",
    "import os\n",
    "\n",
    "# Numerical package (we are importing numpy using the alias np)\n",
    "...\n",
    "\n",
    "# From Matplotlib we will need PyPlot (using the alias plt)\n",
    "...\n",
    "\n",
    "# Open-source Python package for exploring, visualizing, and analyzing human neurophysiological data\n",
    "import mne\n",
    "\n",
    "# mne.viz.set_3d_backend(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d9848-8d90-42ea-a8f6-73d9803ea884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your installation\n",
    "mne.sys_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb10c4c-8a6f-4afd-b9ee-535c0bd3ce97",
   "metadata": {},
   "source": [
    "If all checks out and you get no errors, let's move on to loading some data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b694e-9af0-47c1-981a-0f327eba1467",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Loading data\n",
    "\n",
    "The first thing to do is to load some data from a local file that should be present in your `EEG_raw` directory. To do that, we will load the file `eyes_closed`. This datasets was recorded, as the title suggests, with the participant sitting on the experimental chair with their eyes closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9e65f-65a3-4f12-aae4-09860451cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the eyes-closed data\n",
    "filename = 'eyes_closed.set'\n",
    "data_dir = os.path.abspath('data')\n",
    "EEG_dir = os.path.join(data_dir, 'EEG_raw')\n",
    "data_file = os.path.join(EEG_dir, filename)\n",
    "montage_file = os.path.join(data_dir, 'ElectrodeLoc.xyz')\n",
    "\n",
    "# Read the EEG data\n",
    "eeg_raw = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e9eba-d5a9-4352-b127-23dc9083c3d2",
   "metadata": {},
   "source": [
    "The data `eeg_raw` is in MNE's `raw` format. This is the basis of our subsequent analyses. A good starting point can be found [here](https://mne.tools/dev/auto_tutorials/raw/10_raw_overview.html).\n",
    "\n",
    "**\\[Exercises\\]** Identify the following parameters of the dataset:\n",
    "* How many channels are there?\n",
    "* What is the _sampling frequency_ of the data?\n",
    "\n",
    "\n",
    "**(Hint)** Use the `info` structure from the RAW EEG object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d66b36-376e-4260-b1d2-b2ea798c1f99",
   "metadata": {},
   "source": [
    "**Answers:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191067c-ae97-4f63-9d84-fc2096ca8028",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Visualizing data\n",
    "\n",
    "Great! If everything went smoothly, you now have loaded the EEG data in the variable `eeg_raw`. Let's visualize the data and quickly inspect the channels.\n",
    "\n",
    "**\\[Exercise\\]** Plot the time series of the first 15 channels starting at 70 seconds until 80 seconds.\n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    There are multiple plotting functions in the [MNE documentation](https://mne.tools/stable/overview/index.html)? Which function should we use?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8658954-7395-46f5-b652-67aa270401cf",
   "metadata": {},
   "source": [
    "**Answers**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced50975-e6de-4477-9be0-7d679f5a480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[VI] Plotting the data, visual inspection...\")\n",
    "print('-'*48)\n",
    "\n",
    "with mne.viz.use_browser_backend('matplotlib'):\n",
    "    fig = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d5137-dac7-499e-9911-71bbe6c292c4",
   "metadata": {},
   "source": [
    "#### EEG sensors (Montage)\n",
    "\n",
    "The EEG sensors with their positions are given in the _montage_ (which can be found in the `info` structure). We will plot the default montage in 2D and visualize the sensors below. In our dataset, the positions of the sensors are given in `mm`; however, MNE assumes that positions are in `meters` instead. This will create conflicts. How can we fix this discrepancy?\n",
    "\n",
    "**\\[Exercises\\]** Follow the steps below to convert the positions of the electrodes from `meters` to `mm`\n",
    "* Get the montage from the `eeg_raw` dataset\n",
    "* Transform the coordinates from `mm` to `meters`\n",
    "* Set the montage of the `eeg_raw` dataset to the corrected one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327c2e8-b604-4cfd-a5a2-9b74537072aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default montage\n",
    "default_montage = ...\n",
    "\n",
    "# Fix the montage size and positions\n",
    "for idx in np.arange(len(default_montage.dig)):\n",
    "    default_montage.dig[idx]['r'] /= 1000\n",
    "\n",
    "# Set the updated montage on the dataset\n",
    "...\n",
    "\n",
    "# Plot the sensors montage\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe31c91-b41e-4cf0-ae2e-333299352dcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Preprocessing\n",
    "\n",
    "Our attention will now turn towards some basic preprocessing steps to make our data usable.\n",
    "\n",
    "Our first step is to remove unwanted channels that hold no useful information. Initially, we will keep all the EEG channels and discard the rest.\n",
    "\n",
    "**\\[Exercises\\]**\n",
    "* Read the documentation to identify ways of marking and removing channels from the dataset\n",
    "* We want to keep only the 64 EEG channels and remove all others. Print the names of the channels you removed. Which channels did you remove? How many channels are left?\n",
    "\n",
    "**(Hint)** During the exploratory phase of your analysis, where you might want to try out the effects of different data cleaning approaches, you should get used to patterns like `raw.copy().filter(...).plot()` if you want to avoid having to re-load data and repeat earlier steps each time you change a computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f40e2-80f2-4be3-8240-e63ad4a8e320",
   "metadata": {},
   "source": [
    "**Answers:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d4cea-5de1-46da-a3e4-4583e16449e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary structure to test our channel removals on\n",
    "eeg_non_EEG = eeg_raw.copy()\n",
    "eeg_corrupt = eeg_raw.copy()\n",
    "\n",
    "# Remove all non-EEG channels (idxs over the 64th channel)\n",
    "...\n",
    "\n",
    "# Remove all channels in the list\n",
    "bad_chans = ['F11', 'F12', 'FT11', 'FT12', 'CB1', 'CB2', 'M1', 'M2']\n",
    "...\n",
    "\n",
    "# Print all the channels that will be removed\n",
    "bad_chans = eeg_non_EEG.ch_names + eeg_corrupt.ch_names\n",
    "print('[-] Removing channels...', bad_chans)\n",
    "\n",
    "# Apply the removal\n",
    "eeg_chrm = eeg_raw.copy()\n",
    "...\n",
    "\n",
    "print('[CH] Remaining {0} channels...'.format(len(eeg_chrm.ch_names)), eeg_chrm.ch_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223f98a-f1fe-44a9-8629-2fbc49813820",
   "metadata": {},
   "source": [
    "#### (a) FILTERING\n",
    "\n",
    "Now that we have removed unwanted channels, our next step is to **filter** the remaining data to remove sources of noise and artifacts. We will remove high frequency components (above $200 Hz$) first. Then, we will filter our data to remove very low frequency components that are responsible for linear trends in our data (usually with a cuttoff frequency of $0.1$ - $1 Hz$). Finally, we will apply what is called a _notch filter_ to remove noise caused from electrical power lines. In the EU, electrical equipment operates at $50 Hz$ and thus noise is caused at the same frequency, whereas in North American countries, this frequency component is higher at $60 Hz$.\n",
    "\n",
    "**Note:** While the details might change across datasets and depend on the analysis you intend to do, the procedure remains largely the same. You can easily generalize the following steps for a different dataset in your own work.\n",
    "\n",
    "**Note #2** A very comprehensive (and highly-technical) overview on filtering can be found in the [MNE docs](https://mne.tools/stable/auto_tutorials/preprocessing/25_background_filtering.html). Keep it for future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c2172-e164-4e32-afa8-545f993944c3",
   "metadata": {},
   "source": [
    "#### a. Low-pass filtering\n",
    "\n",
    "Our first step is to apply a low-pass filter on our data and discard frequencies above 200 Hz. You have learned how filtering works during our lectures, so we won't go into more details here.\n",
    "\n",
    "#### b. High-pass filtering\n",
    "\n",
    "A high-pass filter allows us to discard the low frequency components that correspond to what is called the DC-offset. This is almost equivalent to _linearly detrending_ the data.\n",
    "\n",
    "#### c. Notch filtering\n",
    "\n",
    "The notch filter will remove any line noise in our data. As stated above, the power lines in the EU operate at $f_0=50 Hz$. We will also remove the first and second harmonics of this signal, which are the frequencies $2f_0 = 100 Hz$ and $3f_0 = 150 Hz$.\n",
    "\n",
    "**\\[Exercises\\]**\n",
    "* Find in the documentation the filtering functions.\n",
    "* Apply a low-pass filter to the EEG channels with a cuttoff frequency of $f_{cl} = 200 Hz$\n",
    "* Apply a high-pass filter to the EEG channels with a cuttoff frequency of $f_{ch} = 1 Hz$\n",
    "* Apply a notch filter to the EEG channels at the frequency $f_{notch} = 50Hz$ and the two first harmonics\n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    For implementation details, you can use the information found in the MNE documentation <a href=\"https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.filter\">here - filter func</a> and <a href=\"https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.notch_filter\">here - notch filt func</a>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44379f7e-b73c-42fb-94b9-f3628a49a2c3",
   "metadata": {},
   "source": [
    "**Answers:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d60e4-ba01-454e-adcf-e7e5f37d3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in memory - required for the filtering\n",
    "eeg_tmp = eeg_chrm.copy()\n",
    "eeg_tmp.load_data()\n",
    "\n",
    "# Low-pass filter\n",
    "print('[LPF] Filtering the data...')\n",
    "print('-'*32)\n",
    "lpf = 200 # Hz\n",
    "eeg_lpf = ...\n",
    "\n",
    "# High-pass filter\n",
    "hpf = 1 # Hz\n",
    "print('[HPF] Filtering the data...')\n",
    "print('-'*32)\n",
    "eeg_hpf = ...\n",
    "\n",
    "# Notch filter\n",
    "print('[Notch] Filtering the data...')\n",
    "print('-'*32)\n",
    "notch_freqs = np.arange(50, 151, 50) # Europe\n",
    "eeg_filtered = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd232a89-3d5f-48b6-a7c5-b993a2a66bd2",
   "metadata": {},
   "source": [
    "#### Unfiltered data\n",
    "\n",
    "Let's plot the unfiltered data, like we did before, followed by the `info` dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dba495-e396-4b9f-8991-1451155a07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mne.viz.use_browser_backend('matplotlib'):\n",
    "    ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fd6e23-46cc-436f-a4e3-83d8ee520a77",
   "metadata": {},
   "source": [
    "#### Filtered data\n",
    "\n",
    "Let's do the same for the filtered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26944c-6865-426b-8655-40861de93b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mne.viz.use_browser_backend('matplotlib'):\n",
    "    ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e93bbb-5d4a-4693-a05d-9ba975bf43a6",
   "metadata": {},
   "source": [
    "Great! Can you see the difference between the pre- and post-filtering traces? There are a few giveaways that the filtering has worked. Can you spot them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addb039a-06d1-43be-a98d-81433250ec6f",
   "metadata": {},
   "source": [
    "#### (b) EPOCHING\n",
    "\n",
    "Now that we have filtered the data, we can move on to the next step - **epoching**! This is just another name to describe the process of cutting the raw data into pieces that contain only the relevant data for our analyses. For this specific dataset, we will do this using the event numbers that are included in the dataset. In the data you loaded, you might have noticed there are three discrete `EVENTS`, marked on the dataset. Events are used to mark points of interest, like when the trial starts or when a stimulus is presented. Events in the dataset are presented in the form of `annotations`. You can easily spot them using the interactive `plot` function of the raw data.\n",
    "\n",
    "Now that we have an idea of how our data looks like, we can make a few observations. For example, you may have noticed that at the beginning the data looks very noisy compared to 60 seconds in the recording. In later steps, we will learn how to cut unwanted parts of our signals that are outside the trial timeline, and filter out unwanted frequency components.\n",
    "\n",
    "\n",
    "The events (and their codes) are:\n",
    "- `100`: system is synchronized and we are ready to execute the trial\n",
    "- `110`: trial start\n",
    "- `101`: trial end\n",
    "\n",
    "Now that you know the events and their meaning, solve the following exercises.\n",
    "\n",
    "**\\[Exercise\\]**\n",
    "* Find the events and note their codes on the dataset.\n",
    "* Extract the events from the dataset.\n",
    "* Remove event `100` since it is not needed\n",
    "* Epoch the data between event `110` and `101`.\n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    We mentioned how our events are included in the form of annotations on the data. How would you extract the `events` (timing and code) from these annotations?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec149dbd-434b-4405-a7b3-7f57a71375a9",
   "metadata": {},
   "source": [
    "**Answers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff009e4-df0d-42ec-91ac-e7f258f805b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[E] Epoching the data...\")\n",
    "annotations = ...\n",
    "\n",
    "# Print the annotations\n",
    "for idx in range(len(annotations)):\n",
    "    print('Annotation ID:', annotations[idx]['description'], ' onset: ', annotations[idx]['onset'], 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4f90b8-72b3-43fe-a043-b11e35795da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not care about event 100; delete the corresponding annotation\n",
    "...\n",
    "\n",
    "# Crop the data\n",
    "eeg_cropped = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85b8c9-4c8f-40d2-a0e3-67f8c470db75",
   "metadata": {},
   "source": [
    "#### (c) DOWNSAMPLING\n",
    "\n",
    "Now that we have filtered our dataset, we can **downsample** the channels to save space and speed up our computations. One thing to always keep in mind when downsampling is to avoid what is called _[aliasing](https://en.wikipedia.org/wiki/Aliasing)_, which is a form of artifact that is introduced when we reduce the samping frequency too much. The following images demonstrate the effect:\n",
    "\n",
    "Full resolution            |  Downsampled\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/31/Moire_pattern_of_bricks.jpg\" width=\"300\" />  |  <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/fb/Moire_pattern_of_bricks_small.jpg\" width=\"300\" />\n",
    "\n",
    "As you may notice, the image on the right is corrupted; we have removed too much information and this leads to the formation of _visual artifacts_. Avoiding aliasing is easy and just a matter of filtering out any high-frequencies we do not need for our analysis (and noise) before applying the downsampling operation.\n",
    "\n",
    "We will use the [Nyquist-Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem) to decide the highest frequency to keep in our data as well as the new sampling frequency. In our case, our highest frequency lies at around 250 Hz, as demonstrated in our PSD example in the filtering section. According to the sampling theorem, our sampling frequency needs to be _at least_ twice as high as our highest frequency. That would make our new sampling frequency equal to $f_{{s}_{new}} = 2*250 Hz = 500 Hz$. In practical applications, it is always best to leave a bit of extra room, so we will use a sampling frequency of $f_{{s}_{new}} = 600 Hz$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479b857-4221-44d6-ad32-a314f425b36f",
   "metadata": {},
   "source": [
    "**\\[Exercises\\]**\n",
    "* Downsample the data at the new sampling frequency\n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    In the documentation, there are examples and the function definitions. Look <a href=\"https://mne.tools/stable/generated/mne.io.Raw.html#mne.io.Raw.resample\">here</a>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e08f1a8-c4d7-4223-9ed3-2747d0e3c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our new sampling frequency\n",
    "fs_new = ... # in Hz\n",
    "\n",
    "# ALWAYS make a copy!\n",
    "eeg_tmp = eeg_cropped.copy()\n",
    "\n",
    "# Resampling operation here\n",
    "eeg_resampled = ...\n",
    "\n",
    "# Print the info of the new object\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9a75a-8679-4aba-97b2-1da156682dcf",
   "metadata": {},
   "source": [
    "#### (d) RE-REFERENCING\n",
    "\n",
    "Measured EEG from each sensor is a measurement of _voltage_, as we have previously mentioned. Therefore, each sensor is giving us a value of `Volts`. One thing that we haven't mentioned however, is how these measurements relate to each other. Specifically, the measured electric potential that each electrode is recording refers to the electrical potential difference at that electrode relative to the reference electrode. This means that for every electrode at any given time point, the measurement is taken based on the _voltage difference_ between the reference and that electrode. Keep in mind that when recording the data **there is no _'best'_ common reference site!**\n",
    "\n",
    "The idea behind re-referencing is to express the voltage at the EEG scalp channels with respect to another, new reference. Our new reference will be the average of all EEG channels. The advantage of average reference is technical and is found in the fact that outward positive and negative currents, summed across an entire (electrically isolated) sphere like the human head, will sum to 0 (by Ohm’s law). More information can be found in [this EEGlab tutorial](https://eeglab.org/tutorials/ConceptsGuide/rereferencing_background.html).\n",
    "\n",
    "In this section, we will transform our data by re-referencing to the average reference. Let's see how we can do that using MNE.\n",
    "\n",
    "**\\[Exercises\\]**\n",
    "* Average the data to the common average. \n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    This can be easily done using the function `set_eeg_reference` from the documentation <a href=\"https://mne.tools/stable/generated/mne.set_eeg_reference.html\">here</a>. Did you set the required parameters correctly?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368de180-b9a2-4650-acca-1d4295a0a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[REF] Re-referencing the data...')\n",
    "print('-'*32)\n",
    "\n",
    "# Using average reference\n",
    "ref_new = ...\n",
    "\n",
    "# ALWAYS make a copy!\n",
    "eeg_tmp = eeg_resampled.copy()\n",
    "eeg_tmp.load_data()\n",
    "\n",
    "# Re-referencing to common average operation here\n",
    "eeg_reref, ref_data = ...\n",
    "\n",
    "# Print the info of the new object\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb6a18-30f3-416f-a867-0465170b063d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (e) ARTIFACT REJECTION\n",
    "\n",
    "We have successfully cleaned up a large portion of our data. However, some artifacts can still be found in our data! For instance, eye blinks are notorious for causing spikes of activity in the EEG, especially to the electrodes closer to the eyes (Fp1 and Fp2, as well as other frontal electrodes).\n",
    "\n",
    "Another well-known artifact in EEG data originates from muscle movement. Muscle contractions are measured using _electromyography_ (EMG) and muscle contractions around the head during EEG recordings produce high-frequency artifacts in the recorded EEG. \n",
    "\n",
    "Eye blinks usually look similar to the example on the left; muscle movement artifacts usually look similar to the example on the right.\n",
    "\n",
    "\n",
    "Eye Blink Artifact Example | Muscle Artifact Example\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"https://neuraldatascience.io/_images/eog_blink.png\" width=\"350\" alt=\"EEG Eye Blink Artifacts\" vspace=\"50\"/>  |  <img src=\"https://neuraldatascience.io/_images/eeg_muscle.png\" width=\"350\" alt=\"EEG Muscle Artifacts\" vspace=\"50\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Artifact removal with Independent Components Analysis (ICA)**\n",
    "\n",
    "ICA is a blind source separation algorithm. In other words, it can take a complex signal and separate it into _mathematically independent_ components. The algorithm and its usefulness is explained in detail [here](https://neuraldatascience.io/7-eeg/erp_artifacts.html#artifact-removal-with-independent-components-analysis-ica).\n",
    "\n",
    "**\\[Exercises\\]**  Perform the ICA decomposition and reject artifacts using the following steps:\n",
    "* Segment the data for ICA. Use 1-second segments.\n",
    "* Create an ICA object, `ica`, which is separate from the EEG data.\n",
    "* Use the ICA object to fit it onto the data. \n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    For running the ICA, there is a great tutorial found <a href=\"https://neuraldatascience.io/7-eeg/erp_artifacts.html#fit-ica-to-data\">here</a>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1e3d5-68b1-44c7-bb13-594325f9f50e",
   "metadata": {},
   "source": [
    "**Answers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88c9d6-ca27-4cc6-ba7b-ab75d8c1fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[ICA] Run the ICA algorithm and exclude bad components...')\n",
    "print('-'*48)\n",
    "\n",
    "# ALWAYS make a copy!\n",
    "eeg_ica = eeg_reref.copy()\n",
    "\n",
    "# Break raw data into 1-sec epochs\n",
    "tstep = ...\n",
    "events_ica = ...\n",
    "epochs_ica = ...\n",
    "\n",
    "# Autoreject noise\n",
    "from autoreject import get_rejection_threshold\n",
    "\n",
    "reject = ...\n",
    "print(reject)\n",
    "\n",
    "# ICA parameters\n",
    "random_state = 42   # ensures that your ICA decomposition produces the same results each time it's run\n",
    "ica_n_components = .99     # Specify n_components as a decimal to set % explained variance\n",
    "\n",
    "# ICA object\n",
    "ica = ...\n",
    "\n",
    "# Fit ICA\n",
    "...\n",
    "\n",
    "# Plot components and sources - uncomment to plot more information\n",
    "# ica.plot_components();\n",
    "# ica.plot_properties(epochs_ica, picks=range(0, 10), psd_args={'fmax': 60});\n",
    "\n",
    "\n",
    "# Identify and remove EOG artifacts from ICA components\n",
    "ica_z_thresh = 1.96    # corresponds to a 5% chance of a correlation that large occurring due to chance\n",
    "ch_EOG = ['FP1', 'F8'] # specify EEG channels that we want to use in place of EOG channels; we use FP1 and F8 (we only need one channel each for blinks and horizontal movements)\n",
    "                       # FP1/FP2 electrodes are above the eyes; F7/F8 are close enough to the sides of the eyes to detect horizontal movements\n",
    "eog_indices, eog_scores = ... # find bads (EOG)\n",
    "\n",
    "# Identify and remove muscle artifacts from ICA components\n",
    "emg_indices, emg_scores = ... # find bads (muscle)\n",
    "\n",
    "# Exclude components\n",
    "ica.exclude = ...\n",
    "\n",
    "# Plot the scores\n",
    "ica.plot_scores(eog_scores);\n",
    "ica.plot_scores(emg_scores);\n",
    "\n",
    "# Cleaned data\n",
    "eeg_clean = eeg_reref.copy()\n",
    "\n",
    "# Apply the ICA on the clean dataset\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7649d4-d7a7-414c-a94d-ccfec073bee7",
   "metadata": {},
   "source": [
    "#### Don't forget to breathe (and save your data)!\n",
    "\n",
    "Whew! That was a lot of effort for cleaning up our data! Now that the pre-processing part is over, it is time to relax (for a second) and make an overview of our procedure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce776e-3b51-4313-a643-5b270b949146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ICA object; you can load it in subsequent runs if you want\n",
    "# ica.save(os.path.join(data_dir, 'eyes_closed_ica.fif', overwrite=True));\n",
    "\n",
    "# Let's keep a backup of the cleaned up EEG dataset\n",
    "eeg_backup = eeg_clean.copy() # save a backup of our cleaned dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047e389-ff2c-421c-a67a-b929aafde731",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Overview\n",
    "\n",
    "Let's go over the procedure quickly. The whole preprocessing pipeline can be summarized in the following steps:\n",
    "\n",
    "1. Importing the data. We used the EEGlab `.set` file format for ease of use.\n",
    "2. Discarding unnecessary channels and cropping the data based on events.\n",
    "3. Filtering the data: (a) lowpass (b) highpass and (c) notch filtering.\n",
    "4. Downsampling to save space\n",
    "5. Using ICA to find and reject organic artifacts such as eye blinks and muscle noise.\n",
    "6. Re-referencing to common average.\n",
    "\n",
    "Great! Now that all of the above is clear, can we make a function to perform all of the above automatically?\n",
    "\n",
    "**\\[Exercises\\]**\n",
    "* Write a function that accepts a filename and a few other key parameters and performs all of the above steps automatically.\n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    Take note of the parameters to be used and read the documentation string in the provided function!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bad1eb6-dfcf-4e82-bad7-cdc32ba21be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_cleanup(fname, reject_chans, freqs_filt, freqs_notch, fs_downsample):\n",
    "    \"\"\"\n",
    "    Read and preprocess EEG data using MNE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: string\n",
    "        Filename to be read. Use `.set` files as we already did\n",
    "    reject_chans: list of strings\n",
    "        Include the _names_ of the channels to be rejected. You can reuse the channel names you rejected previously\n",
    "    freqs_filt: tuple\n",
    "        Filtering frequencies. This should be a tuple in the form (flow, fhigh) with flow<fhigh\n",
    "    freqs_notch: tuple\n",
    "        Notch filter frequencies. Give the filtered frequencies and their first/second harmonics.\n",
    "    fs_downsample: int\n",
    "        Downsampling frequency to be used for resampling the data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    eeg_data: MNE in\n",
    "        Modulation Index, as computed in the paper by Tort et al. (2010)\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the EEG data\n",
    "    print('[1] Reading data...')\n",
    "    ...\n",
    "    \n",
    "    # Adjust the montage\n",
    "    # Get the default montage\n",
    "    ...\n",
    "    \n",
    "    # Fix the montage size and positions\n",
    "    ...\n",
    "    \n",
    "    # Reject channels\n",
    "    print('[2] Rejecting channels...')\n",
    "    ...\n",
    "    \n",
    "    # Crop the data (event-based)\n",
    "    print('[3] Cropping...')\n",
    "    ...\n",
    "    \n",
    "    # We do not care about event 100\n",
    "    ...\n",
    "    \n",
    "    # Filtering\n",
    "    print('[4] Filtering...')\n",
    "    print('(a) Low-pass filtering')\n",
    "    ...\n",
    "    \n",
    "    print('(b) High-pass filtering')\n",
    "    ...\n",
    "    \n",
    "    print('(a) Notch filtering')\n",
    "    ...\n",
    "\n",
    "    # Downsampling\n",
    "    print('[5] Downsampling...')\n",
    "    ...\n",
    "    \n",
    "    # Artifact rejection\n",
    "    print('[6] Artifact cleanup...')\n",
    "    ...\n",
    "    \n",
    "    # ICA parameters\n",
    "    random_state = 42   # ensures that your ICA decomposition produces the same results each time it's run\n",
    "    ica_n_components = .99     # Specify n_components as a decimal to set % explained variance\n",
    "    \n",
    "    # Autoreject noise\n",
    "    from autoreject import get_rejection_threshold\n",
    "    print('(a) Autoreject (initial step)')\n",
    "    \n",
    "    # Break raw data into 1-sec epochs\n",
    "    ...\n",
    "    \n",
    "    # ICA object\n",
    "    ...\n",
    "    \n",
    "    # Fit ICA\n",
    "    ...\n",
    "    \n",
    "    # Identify and remove EOG artifacts from ICA components\n",
    "    ...\n",
    "    \n",
    "    # Identify and remove muscle artifacts from ICA components\n",
    "    ...\n",
    "    \n",
    "    # Exclude components\n",
    "    ...\n",
    "    \n",
    "    # Cleaned data\n",
    "    ...\n",
    "    \n",
    "    # Done! Return the data\n",
    "    print('[*] Done!')\n",
    "    return eeg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09235d3-9073-47bc-80d0-94b2837dc6cb",
   "metadata": {},
   "source": [
    "Good job! Now you should have a function to automatically read and preprocess EEG datasets. Let's test it out by reading the second dataset named `eyes_open.set`.\n",
    "\n",
    "**\\[Exercises\\]**\n",
    "* Use the function you wrote previously to read the dataset from the file `eyes_open.set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17a668-e800-4de2-9cdd-50c152cb39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'eyes_open.set'\n",
    "data_file = os.path.join(EEG_dir, filename)\n",
    "\n",
    "bad_chans = ['F11', 'F12', 'FT11', 'FT12', 'CB1', 'CB2']\n",
    "eeg_eyes_open = automate_cleanup(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d751fa-2f53-4660-9e98-0120f8c68ff0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 7. A simple analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f22d7d-4948-4207-aac0-a36b1bfbc815",
   "metadata": {},
   "source": [
    "#### POWER SPECTRAL DENSITY (PSD)\n",
    "\n",
    "The power spectral density (PSD) describes the distribution of power into frequency components composing that signal. For a mathematical definition (not required for this class) you can refer to the [Wiki page](https://en.wikipedia.org/wiki/Spectral_density).\n",
    "\n",
    "To calculate the power spectral density, we will use [Welch's method](https://en.wikipedia.org/wiki/Welch%27s_method). A practical example and an explanation using SciPy can be found [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.welch.html). Note that you can compute the PSD in whichever way you prefer, but the advised way is to use MNE's toolbox to compute and plot it.\n",
    "\n",
    "While we won't go into too much detail about how Welch's average periodogram works, the parameters that you will use have already been given to you in the following cell in the form of a _dictionary_ that you can pass in the MNE function to compute the PSD estimate. Can you see the difference each of them makes on the output?\n",
    "\n",
    "**\\[Exercise\\]** Calculate and plot the PSD of the data. What do you observe?\n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    In MNE 1.2.0 you should use the function `compute_psd()` to calculate the PSD and then followed by `plot()` to plot it. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44bdce6-a9a5-4eb4-9eec-6020f5c15050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to compute the PSD\n",
    "window_size = 2 # second\n",
    "winsize_samples = ... # samples; how many samples are there in a 2-second window? how can you find the sampling frequency of the data?\n",
    "\n",
    "# Window overlap - reduces noise\n",
    "overlap = 0.9 # percentage\n",
    "noverlap_samples = ... # samples; how many samples do we need to set as overlapping samples to achieve 90% window overlap? \n",
    "\n",
    "# Make the parameters dictionary for computing and plotting the PSD\n",
    "psd_parameters = dict(method='welch', fmax=60, n_fft=winsize_samples, n_overlap=noverlap_samples)\n",
    "\n",
    "# Compute the PSD of the `eyes_closed` dataset and plot it\n",
    "... # which function computes the PSD in the MNE toolbox? \n",
    "... # how can you plot the data?\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c745eb5-dc11-4e2e-bde5-2a6d999e0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the PSD and plot it\n",
    "... # same as above\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58304d02-4d81-4df0-8200-62de08e926a2",
   "metadata": {},
   "source": [
    "As a sidenote, we can compute the PSD over specific groups of sensors. Let's take the following cases:\n",
    "\n",
    "1. Frontal vs Posterior\n",
    "2. Left vs Right\n",
    "\n",
    "EEG sensors are named after their location and numbered according to their left-right-ness. Frontal electrodes names start with an 'F'. Central electrodes start with a 'C'. Posterior electrodes start with 'P'. To get the sensors on the _left_ side of the head, you get the channel names that end in an odd number (1,3,5,...). To get the sensors on the _right_ side of the head, you get the even-numbered channels (0,2,4,...).\n",
    "\n",
    "**\\[Exercises\\]** Using the information above, calculate the following:\n",
    "* Group the sensors according to their groups (frontal vs posterior and left vs right).\n",
    "* Calculate and plot the PSDs\n",
    "* At what frequency is the prominent peak located?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70ad2d-722f-44a0-b095-ececa8c54a9c",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494bf82-d233-4329-bcae-23652d159210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frontal\n",
    "regexp_f = r'^[fFaA].*' # regular expression; you can use the function pick_channels_regexp\n",
    "front_idx = ...\n",
    "\n",
    "# Posterior\n",
    "regexp_p = r'^[pPoO].*'\n",
    "post_idx = ...\n",
    "\n",
    "# Left\n",
    "regexp_l = r'\\w*[13579]$'\n",
    "left_idx = ...\n",
    "\n",
    "# Right\n",
    "regexp_r = r'\\w*[02468]$'\n",
    "right_idx = ...\n",
    "\n",
    "# Average per area\n",
    "groups = dict(Front=front_idx, Post=post_idx, Left=left_idx, Right=right_idx) # make a dictionary as required in the combine_channels function\n",
    "\n",
    "# Average traces - combine channels\n",
    "eeg_avg_all = ...\n",
    "\n",
    "# Compute the PSD and plot it\n",
    "...\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a27744-6e62-48e8-b2c7-fc876abc87d7",
   "metadata": {},
   "source": [
    "### 8. Comparison of eyes open vs eyes closed\n",
    "\n",
    "Great! Now that we have a pipeline in place for reading and cleaning our data, let's try to compare two different datasets.\n",
    "\n",
    "**\\[Exercises\\]** Read the dataset `eyes_closed.set` and run the analysis pipeline.\n",
    "* Calculate and plot the PSD.\n",
    "* Compare the `eyes_closed` to the `eyes_open` conditions. What do you see? Is there a particular rhythm more prevalent in one of the conditions?\n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    You are looking for a difference in the PSDs in the form of a prevalent peak around a certain frequency. In the literature, you can easily find the main difference between EEG rhythms for eyes open and eyes closed.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96205c-f61b-454c-9d83-4819d88a788e",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de060178-6334-4908-8ba8-c15eaf0aa149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a copy of the previously-analyzed data\n",
    "eeg_eyes_closed = eeg_backup.copy()\n",
    "\n",
    "# Average all channels per condition\n",
    "groups = ... # we made a dictionary above to select channels based on their location; let's make a new one for All channels\n",
    "\n",
    "# Compute the PSDs of the two conditions\n",
    "eeg_eyes_closed_PSD = ...\n",
    "eeg_eyes_open_PSD = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c044a29-5f83-47ba-adb5-08978a036609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data (y) and the frequency (x) for plotting\n",
    "data_eyes_open, freqs_eyes_open = ... # how can you ~get~ the ~data~, including the frequency axes?\n",
    "data_eyes_closed, freqs_eyes_closed = ...\n",
    "\n",
    "# Calculate the mean data by averaging the PSDs onto a single trace\n",
    "data_eyes_open_avg = ...\n",
    "data_eyes_closed_avg = ...\n",
    "\n",
    "# Plot the traces using a semi-logarithmic y-axis\n",
    "plt.semilogy(..., label='eyes open') # linear x, logarithmic y axes\n",
    "plt.semilogy(..., label='eyes closed')\n",
    "\n",
    "# We add a vertical line that denotes the peak\n",
    "plt.axvline(x=11, ls='--', c='grey', lw=0.75)\n",
    "\n",
    "# Show the legend and add a grid\n",
    "...\n",
    "\n",
    "# Add titles and x-y labels\n",
    "...\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035dc3fe-1ba6-432e-b94f-446b4214aa0f",
   "metadata": {},
   "source": [
    "Great! Now that we have plotted the two averaged PSDs, what do you notice? Do you see the peak in the `eyes_closed` condition? At what frequency?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae341c-dea2-4a4b-a640-537baebbeb34",
   "metadata": {},
   "source": [
    "**BONUS**\n",
    "\n",
    "**\\[Exercises\\]** To get the power in the EEG band of interest (alpha), we will use the provided function `bandpower()` given below. Can you find the absolute difference in alpha power between the two datasets?\n",
    "\n",
    "<details>\n",
    "    <summary> <b> (Hint) </b> </summary>\n",
    "    You should have already calculated the power spectral densities. How can you use the computed PSDs to get the bandpower? <b>Do not forget to run the cell containing the function!</b>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75848ba2-bf78-4f82-b565-a266554e5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the power spectral density of the alpha band per condition\n",
    "def bandpower(psd, freqs, band, relative=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Compute the average power of the signal x in a specific frequency band.\n",
    "    Code adapted from: https://raphaelvallat.com/bandpower.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: 1d-array\n",
    "        Input signal in the time-domain.\n",
    "    fs: float\n",
    "        Sampling frequency of the data.\n",
    "    band: list\n",
    "        Lower and upper frequencies of the band of interest.\n",
    "    window_sec: float\n",
    "        Length of each window in seconds.\n",
    "        If None, window_sec = (1 / min(band)) * 2\n",
    "    relative: boolean\n",
    "        If True, return the relative power (= divided by the total power of the signal).\n",
    "        If False (default), return the absolute power.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bp : float\n",
    "        Absolute or relative band power.\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    band = np.asarray(band)\n",
    "    low, high = band\n",
    "\n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "\n",
    "    # Find closest indices of band in frequency vector\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "\n",
    "    # Integral approximation of the spectrum using Simpson's rule.\n",
    "    bp = simps(psd[idx_band], dx=freq_res)\n",
    "\n",
    "    if relative:\n",
    "        bp /= simps(psd, dx=freq_res)\n",
    "\n",
    "    return bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c73d9e-169d-46d1-8029-d8b0b4df1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of the alpha band\n",
    "alpha_band = ...\n",
    "\n",
    "# Calculate the power\n",
    "abp_eyes_open = ...\n",
    "abp_eyes_closed = ...\n",
    "\n",
    "print(r'$\\alpha$ power for eyes_open case:', round(abp_eyes_open/(10e-12), 6), r'uV^2/Hz')\n",
    "print(r'$\\alpha$ power for eyes_closed case', round(abp_eyes_closed/(10e-12), 6), r'uV^2/Hz')\n",
    "print(r'$\\alpha$ power difference:', abs(round(abp_eyes_open/(10e-12) - abp_eyes_closed/(10e-12), 6)), r'uV^2/Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7562cf6-9ca3-4214-bf58-c3484477ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the traces\n",
    "... # linear x, y axes\n",
    "\n",
    "# We add a vertical line that denotes the peak\n",
    "...\n",
    "\n",
    "# Show the legend and add a grid\n",
    "...\n",
    "\n",
    "# Add titles and x-y labels\n",
    "plt.title('PSD Comparison')\n",
    "plt.xlabel('Frequencies [Hz]')\n",
    "plt.ylabel('Power spectral density (uV^2 / Hz)')\n",
    "\n",
    "# Find intersecting values in frequency vector\n",
    "...\n",
    "\n",
    "# Fill areas below curve\n",
    "...\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee690809-e18d-446d-8544-64bbe2159a4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9. Outro\n",
    "\n",
    "Great work completing the EEG project! We hope you learned the basics of (pre-)processing EEG data and encourage you to play more with the pipeline you developed and personalize it on your own datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
